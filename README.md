<h1 style="text-align: center;">City of Toronto Parks, Forestry & Recreation</h1>
<h1 style="text-align: center;">Take-Home Assignment for Data Specialist (Job Id: 25235)</h1>

<h5 style="text-align: center;">Notebook by Zhanyang (Andy) Gong</h5>
<h5 style="text-align: center;">Email: Gongzhanyang@gmail.com</h5>
<h5 style="text-align: center;">Phone: 647-570-5520</h5>

**It is recommended to view [this notebook in nbviewer](https://nbviewer.org/github/AndyGongDS/CoT_Take_Home/blob/7c44d032fad4359b4e5dff5b3d4eca6ec4a7bdeb/notebooks/Take%20Home%20Assignment.ipynb) or PDF for the best viewing experience.**


This notebook was prepared by Zhanyang (Andy) Gong, for the Take-Home Assignment from the City of Toronto Parks, Forestry & Recreation. The notebook was structured in six (6) different sections, covering all the topics listed in the Delivery Checklist table. A Table of Contents below has listed each of the sections. The titles can be clicked and directed to the corresponding section. A [Back to Top] button has been placed at the end of each section, for clicking and directing back to the Table of Contents.


Throughout the notebook, different approaches in wrangling, organizing, and cleaning were used for raw data downloaded from the City of Toronto Public Data Platform. Functions, loops, pipelines were created to automate some of the processes so that repeated work could be completed efficiently. 

A few data visualization tools have been used in this assignment, including Seaborn, Matplotlib (for regular data presentation), and Folium (for Geospatial Data Visualization).


In terms of Geospatial Analysis, I used a few API tools to geocode addresses so that they can be located in the customized maps. Interactive maps have been created in Section 5 to present the locations, data, and analyzing results for the assignment. 


The notebook was published on my private GitHub as a repository. The work can be shared and collaboration can be done through GitHub if needed. 

The analysis also involved an Unsupervised Machine Learning model, published in Scikit-Learning, in order to efficiently deal with extremely large amounts of data processing, and it greatly decreased the processing time. 